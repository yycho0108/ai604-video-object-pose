{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for core lib\n",
    "import torch as th\n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for visualization\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'objectron_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-de81ce63357a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Imports for dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m from objectron_dataset import (Objectron, SampleObjectron,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                DecodeImage, ParseFixedLength, _skip_none)\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maugment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhotometricAugment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'objectron_dataset'"
     ]
    }
   ],
   "source": [
    "# Imports for dataset\n",
    "from objectron_dataset import (Objectron, SampleObjectron,\n",
    "                               DecodeImage, ParseFixedLength, _skip_none)\n",
    "from augment import PhotometricAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure experiment\n",
    "batch_size = 2\n",
    "image_shape = (480,640)\n",
    "sequence_length = 8\n",
    "num_workers = 0\n",
    "use_cached_samples = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure options, transforms\n",
    "# instantiate dataset and loader\n",
    "obj_cls = SampleObjectron if use_cached_samples else Objectron\n",
    "opts = obj_cls.Settings()\n",
    "xfm = transforms.Compose([\n",
    "    DecodeImage(size=image_shape),\n",
    "    ParseFixedLength(ParseFixedLength.Settings()),\n",
    "    PhotometricAugment(PhotometricAugment.Settings())\n",
    "])\n",
    "dataset = obj_cls(opts, xfm)\n",
    "loader = th.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, num_workers=num_workers,\n",
    "    collate_fn=_skip_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get single data from loader. Takes a VERY long time.\n",
    "data = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts, features = data\n",
    "features['object/class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_objects_with_pose(feat):\n",
    "    image = feat['image'].cpu().numpy()\n",
    "    _, h, w = image.shape\n",
    "    \n",
    "    # NOTE(ycho): copy required due to OpenCV restrictions (contiguous)\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    image = image.copy()\n",
    "    \n",
    "    # 3D Bounding-box vertices.\n",
    "    box_verts = [\n",
    "        (-0.5,-0.5,-0.5),\n",
    "        (-0.5,-0.5,+0.5),\n",
    "        (-0.5,+0.5,-0.5),\n",
    "        (-0.5,+0.5,+0.5),\n",
    "        (+0.5,-0.5,-0.5),\n",
    "        (+0.5,-0.5,+0.5),\n",
    "        (+0.5,+0.5,-0.5),\n",
    "        (+0.5,+0.5,+0.5),\n",
    "    ]\n",
    "    box_verts = np.asarray(box_verts, dtype=np.float32)\n",
    "    \n",
    "    num_inst = feat['instance_num'][0]\n",
    "    for i in range(num_inst):\n",
    "        irxn = feat['object/orientation'][i*9:(i+1)*9]\n",
    "        itxn = feat['object/translation'][i*3:(i+1)*3]\n",
    "        iscale = feat['object/scale'][i*3:(i+1)*3]\n",
    "        \n",
    "        T_scale = np.diag(np.r_[iscale.cpu().numpy(), 1.0])\n",
    "        # BBOX3D transform\n",
    "        T_box = np.eye(4)\n",
    "        T_box[:3,:3] = irxn.reshape(3,3).cpu().numpy()\n",
    "        T_box[:3,-1] = itxn.cpu().numpy()\n",
    "        \n",
    "        # camera transforms\n",
    "        T_p = feat['camera/projection'].cpu().numpy().reshape(4,4)\n",
    "#         T_v = feat['camera/view'].cpu().numpy().reshape(4,4)\n",
    "        \n",
    "        # Compose all transforms\n",
    "        # NOTE(ycho): Looks like `camera/view` is not needed.\n",
    "        # Perhaps it's been fused into object/{translation,orientation}.\n",
    "        T = T_p @ T_box @ T_scale\n",
    "        \n",
    "        # apply transform\n",
    "        v = box_verts @ T[:3,:3].T + T[:3,-1]\n",
    "        # project\n",
    "        v[..., :-1] /= v[..., -1:]\n",
    "        \n",
    "        # TODO(ycho): Consider also incorporating\n",
    "        # NDC transform into the above composed xfm.\n",
    "        v[...,0] = (1 + v[...,0]) * (0.5 * h)\n",
    "        v[...,1] = (1 + v[...,1]) * (0.5 * w)\n",
    "        y, x = v[...,0], v[..., 1]\n",
    "        \n",
    "        for (px, py) in zip(x,y):\n",
    "            cv2.circle(image, (int(px),int(py)), 16, (0,0,255), -1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO(ycho): Deal with cases where collation results < batch_size samples\n",
    "# due to short sequence or network error. For now, no such error checking is performed.\n",
    "fig, axs = plt.subplots(batch_size, sequence_length, figsize=(12,8), dpi= 200)\n",
    "for i_batch in range(batch_size):\n",
    "    for i_seq in range(sequence_length):\n",
    "#         print({k:v.shape for (k,v) in features.items()})\n",
    "        ctx = {k : v[i_batch] for (k,v) in contexts.items()}\n",
    "        feat = {k : v[i_batch,i_seq] for (k,v) in features.items()}\n",
    "        image = draw_objects_with_pose(feat)\n",
    "        \n",
    "        axs[i_batch,i_seq].imshow(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "th_env",
   "language": "python",
   "name": "th_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
